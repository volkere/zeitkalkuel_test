
import io, json, tempfile
from typing import List, Dict, Any
import streamlit as st
import numpy as np
from PIL import Image
import cv2

from app.face_recognizer import FaceEngine, GalleryDB
from app.location import extract_exif_gps, reverse_geocode, extract_comprehensive_metadata, get_location_details

st.title("ğŸ–¼ï¸ Annotate: Fotos analysieren")
st.caption("Erweiterte Gesichtserkennung, Metadaten-Extraktion und Standortanalyse")

# BenutzerfÃ¼hrung
with st.expander("â„¹ï¸ Anleitung", expanded=False):
    st.markdown("""
    **So verwenden Sie diese Seite:**
    
    1. **ğŸ“ Bilder hochladen**: WÃ¤hlen Sie Bilder in der Sidebar aus
    2. **âš™ï¸ Einstellungen anpassen**: Konfigurieren Sie die Erkennungsparameter
    3. **ğŸ”„ Verarbeitung**: Die App analysiert automatisch alle Bilder
    4. **â¬‡ï¸ Download**: Nach der Verarbeitung erscheint ein Download-Button am Ende
    5. **ğŸ“Š Analyse**: Laden Sie die JSON-Datei in der 'Analyze'-Seite hoch
    
    **ğŸ’¡ Tipp**: Der Download-Button erscheint erst nach der Verarbeitung aller Bilder!
    """)

with st.sidebar:
    st.header("âš™ï¸ Einstellungen")
    
    # Gesichtserkennung
    st.subheader("Gesichtserkennung")
    det = st.slider("Detector size", 320, 1024, 640, 64, key="det_annot")
    threshold = st.slider("Identity threshold (cosine)", 0.3, 0.9, 0.55, 0.01)
    
    # Metadaten
    st.subheader("Metadaten")
    extract_full_metadata = st.checkbox("VollstÃ¤ndige EXIF-Metadaten extrahieren", value=True)
    do_reverse = st.checkbox("Reverse geocode GPS (Internet)", value=False)
    show_location_details = st.checkbox("Detaillierte Standortinfos", value=False)
    
    # QualitÃ¤tsfilter
    st.subheader("QualitÃ¤tsfilter")
    min_quality = st.slider("Min. GesichtsqualitÃ¤t", 0.0, 1.0, 0.3, 0.1)
    min_face_size = st.slider("Min. GesichtsgrÃ¶ÃŸe (Pixel)", 50, 200, 80, 10)
    
    # Datei-Upload
    st.subheader("Dateien")
    gallery_file = st.file_uploader("Embeddings DB (embeddings.pkl)", type=["pkl"], key="db_upload")
    files = st.file_uploader("Bilder hochladen", type=["jpg","jpeg","png","bmp","webp","tif","tiff"], accept_multiple_files=True)

if "engine_annot" not in st.session_state or st.session_state.get("det_annot_state") != det:
    st.session_state["engine_annot"] = FaceEngine(det_size=(det, det))
    st.session_state["det_annot_state"] = det

db = None
if gallery_file is not None:
    import pickle
    try:
        db = GalleryDB()
        data = pickle.load(gallery_file)
        if isinstance(data, dict):
            db.people = data.get('people', {})
            db.face_metadata = data.get('metadata', {})
        else:
            db.people = data
        st.success(f"Embeddings geladen: {len(db.people)} Personen.")
    except Exception as e:
        st.error(f"Fehler beim Laden der Embeddings: {e}")

def draw_boxes(img_bgr, persons):
    img = img_bgr.copy()
    for p in persons:
        x1,y1,x2,y2 = map(int, p["bbox"])
        
        # Farbe basierend auf QualitÃ¤t
        quality = p.get('quality_score', 0.5)
        if quality > 0.7:
            color = (0, 255, 0)  # GrÃ¼n fÃ¼r hohe QualitÃ¤t
        elif quality > 0.4:
            color = (0, 255, 255)  # Gelb fÃ¼r mittlere QualitÃ¤t
        else:
            color = (0, 0, 255)  # Rot fÃ¼r niedrige QualitÃ¤t
        
        cv2.rectangle(img, (x1,y1), (x2,y2), color, 2)
        
        # Label mit erweiterten Informationen
        label_parts = []
        
        # Name und Ã„hnlichkeit
        if p.get("name"):
            sim = f" ({p['similarity']:.2f})" if p.get("similarity") is not None else ""
            label_parts.append(p["name"] + sim)
        
        # Demografie
        if p.get("gender"):
            label_parts.append(p["gender"])
        if p.get("age") is not None:
            label_parts.append(f"{p['age']}J")
        
        # QualitÃ¤t
        if p.get("quality_score"):
            label_parts.append(f"Q:{p['quality_score']:.2f}")
        
        # Emotion
        if p.get("emotion"):
            label_parts.append(p["emotion"])
        
        # Augen/Mund Status
        status_parts = []
        if p.get("eye_status"):
            status_parts.append(f"ğŸ‘:{p['eye_status']}")
        if p.get("mouth_status"):
            status_parts.append(f"ğŸ‘„:{p['mouth_status']}")
        
        if status_parts:
            label_parts.append(" ".join(status_parts))
        
        txt = " | ".join(label_parts) if label_parts else f"{p.get('prob', 1.0):.2f}"
        
        # Text-Hintergrund fÃ¼r bessere Lesbarkeit
        (text_width, text_height), _ = cv2.getTextSize(txt, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
        cv2.rectangle(img, (x1, max(0,y1-text_height-8)), (x1+text_width, y1), color, -1)
        cv2.putText(img, txt, (x1, max(0,y1-8)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA)
    
    return img

def display_metadata_card(metadata, title="Metadaten"):
    """Zeigt Metadaten in einer schÃ¶nen Karte an"""
    with st.expander(f"ğŸ“Š {title}", expanded=False):
        if not metadata:
            st.info("Keine Metadaten verfÃ¼gbar")
            return
        
        # Kamera-Informationen
        if any(key in metadata for key in ['camera_make', 'camera_model', 'lens']):
            st.subheader("ğŸ“· Kamera")
            col1, col2, col3 = st.columns(3)
            with col1:
                if metadata.get('camera_make'):
                    st.metric("Hersteller", metadata['camera_make'])
            with col2:
                if metadata.get('camera_model'):
                    st.metric("Modell", metadata['camera_model'])
            with col3:
                if metadata.get('lens'):
                    st.metric("Objektiv", metadata['lens'])
        
        # Aufnahme-Einstellungen
        if any(key in metadata for key in ['focal_length', 'f_number', 'iso', 'exposure_time']):
            st.subheader("âš™ï¸ Aufnahme-Einstellungen")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                if metadata.get('focal_length'):
                    st.metric("Brennweite", f"{metadata['focal_length']}mm")
            with col2:
                if metadata.get('f_number'):
                    st.metric("Blende", f"f/{metadata['f_number']}")
            with col3:
                if metadata.get('iso'):
                    st.metric("ISO", metadata['iso'])
            with col4:
                if metadata.get('exposure_time'):
                    st.metric("Belichtung", f"1/{metadata['exposure_time']}s")
        
        # Datum und Zeit
        if metadata.get('datetime'):
            st.subheader("ğŸ•’ Aufnahmezeit")
            st.info(f"**{metadata['datetime']}**")
        
        # GPS und Standort
        if metadata.get('gps'):
            st.subheader("ğŸ“ Standort")
            gps = metadata['gps']
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Breitengrad", f"{gps['lat']:.6f}")
            with col2:
                st.metric("LÃ¤ngengrad", f"{gps['lon']:.6f}")
            
            if gps.get('altitude'):
                st.metric("HÃ¶he", f"{gps['altitude']:.1f}m")
            
            if gps.get('timestamp'):
                st.info(f"GPS-Zeitstempel: {gps['timestamp']}")
        
        # BildgrÃ¶ÃŸe
        if metadata.get('image_width') and metadata.get('image_height'):
            st.subheader("ğŸ“ BildgrÃ¶ÃŸe")
            st.metric("AuflÃ¶sung", f"{metadata['image_width']} Ã— {metadata['image_height']} Pixel")

def display_face_analysis(persons):
    """Zeigt detaillierte Gesichtsanalyse an"""
    if not persons:
        st.info("Keine Gesichter erkannt")
        return
    
    st.subheader("ğŸ‘¥ Gesichtsanalyse")
    
    for i, person in enumerate(persons):
        with st.expander(f"Person {i+1}", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                # Basis-Informationen
                st.write("**Identifikation:**")
                if person.get("name"):
                    st.success(f"âœ… {person['name']} (Ã„hnlichkeit: {person.get('similarity', 0):.2f})")
                else:
                    st.warning("â“ Unbekannte Person")
                
                # Demografie
                st.write("**Demografie:**")
                if person.get("age"):
                    st.write(f"Alter: {person['age']} Jahre")
                if person.get("gender"):
                    st.write(f"Geschlecht: {person['gender']}")
                
                # QualitÃ¤t
                if person.get("quality_score"):
                    quality = person['quality_score']
                    st.write("**QualitÃ¤t:**")
                    if quality > 0.7:
                        st.success(f"Hohe QualitÃ¤t ({quality:.2f})")
                    elif quality > 0.4:
                        st.warning(f"Mittlere QualitÃ¤t ({quality:.2f})")
                    else:
                        st.error(f"Niedrige QualitÃ¤t ({quality:.2f})")
            
            with col2:
                # Emotion und Status
                st.write("**Gesichtsausdruck:**")
                if person.get("emotion"):
                    emotion_icons = {"happy": "ğŸ˜Š", "neutral": "ğŸ˜", "unknown": "â“"}
                    icon = emotion_icons.get(person["emotion"], "â“")
                    st.write(f"{icon} {person['emotion']}")
                
                # Augen-Status
                if person.get("eye_status"):
                    eye_icons = {"open": "ğŸ‘ï¸", "partially_open": "ğŸ˜‘", "closed": "ğŸ˜´"}
                    icon = eye_icons.get(person["eye_status"], "ğŸ‘ï¸")
                    st.write(f"{icon} Augen: {person['eye_status']}")
                
                # Mund-Status
                if person.get("mouth_status"):
                    mouth_icons = {"open": "ğŸ˜®", "closed": "ğŸ˜"}
                    icon = mouth_icons.get(person["mouth_status"], "ğŸ˜")
                    st.write(f"{icon} Mund: {person['mouth_status']}")

results: List[Dict[str, Any]] = []

if files:
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, up in enumerate(files):
        status_text.text(f"Verarbeite {up.name}...")
        
        data = up.read()
        image = Image.open(io.BytesIO(data)).convert("RGB")
        img_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

        # Gesichtserkennung
        faces = st.session_state["engine_annot"].analyze(img_bgr)
        
        # QualitÃ¤tsfilter anwenden
        filtered_faces = []
        for f in faces:
            face_size = (f['bbox'][2] - f['bbox'][0]) * (f['bbox'][3] - f['bbox'][1])
            if (f.get('quality_score', 0.5) >= min_quality and 
                face_size >= min_face_size):
                filtered_faces.append(f)
        
        persons = []
        for f in filtered_faces:
            name, sim = (None, None)
            if db:
                n, s = db.match(f["embedding"], threshold=threshold)
                name, sim = (n, s)
            persons.append({
                "bbox": f["bbox"],
                "prob": f["prob"],
                "name": name,
                "similarity": sim,
                "age": f["age"],
                "gender": f["gender"],
                "quality_score": f.get("quality_score"),
                "emotion": f.get("emotion"),
                "eye_status": f.get("eye_status"),
                "mouth_status": f.get("mouth_status")
            })

        # Metadaten-Extraktion
        with tempfile.NamedTemporaryFile(suffix=".jpg", delete=True) as tmp:
            image.save(tmp.name, format="JPEG")
            
            if extract_full_metadata:
                metadata = extract_comprehensive_metadata(tmp.name)
                gps_data = metadata.get('gps')
            else:
                metadata = {}
                gps_data = extract_exif_gps(tmp.name)
                if gps_data:
                    metadata['gps'] = gps_data
        
        # Standort-Informationen
        location_info = None
        if gps_data and do_reverse:
            if show_location_details:
                location_info = get_location_details(gps_data['lat'], gps_data['lon'])
            else:
                address = reverse_geocode(gps_data['lat'], gps_data['lon'])
                if address:
                    location_info = {'full_address': address}

        record = {
            "image": up.name,
            "metadata": metadata,
            "location": location_info,
            "persons": persons
        }
        results.append(record)

        # Anzeige
        st.header(f"ğŸ“¸ {up.name}")
        
        # Bildanzeige
        col1, col2 = st.columns(2)
        with col1:
            st.image(image, caption="Original", use_container_width=True)
        with col2:
            boxed = draw_boxes(img_bgr, persons)
            st.image(cv2.cvtColor(boxed, cv2.COLOR_BGR2RGB), caption="Erkannte Gesichter", use_container_width=True)
        
        # Metadaten anzeigen
        display_metadata_card(metadata, "ğŸ“Š Bild-Metadaten")
        
        # Standort anzeigen
        if location_info:
            with st.expander("ğŸ“ Standort-Informationen", expanded=False):
                if location_info.get('full_address'):
                    st.info(f"**Adresse:** {location_info['full_address']}")
                
                if location_info.get('country'):
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        if location_info.get('country'):
                            st.metric("Land", location_info['country'])
                    with col2:
                        if location_info.get('state'):
                            st.metric("Bundesland", location_info['state'])
                    with col3:
                        if location_info.get('city'):
                            st.metric("Stadt", location_info['city'])
        
        # Gesichtsanalyse anzeigen
        display_face_analysis(persons)
        
        # JSON-Export (kollabiert)
        with st.expander("ğŸ“„ JSON-Daten", expanded=False):
            st.json(record)
        
        st.divider()
        
        # Fortschritt aktualisieren
        progress_bar.progress((idx + 1) / len(files))
    
    status_text.text("Verarbeitung abgeschlossen!")
    
    # Download-Button fÃ¼r alle Ergebnisse
    st.success(f"âœ… {len(results)} Bilder erfolgreich verarbeitet")
    
    # Download-Button
    col1, col2 = st.columns(2)
    with col1:
        st.download_button("â¬‡ï¸ Download results JSON",
                           data=json.dumps(results, ensure_ascii=False, indent=2),
                           file_name="results.json",
                           mime="application/json")
    with col2:
        st.info("ğŸ’¡ Tipp: Laden Sie diese JSON-Datei in der 'Analyze'-Seite hoch fÃ¼r erweiterte Statistiken!")
else:
    st.info("ğŸ“ Bilder in der Sidebar hochladen, um zu starten.")
    
    # Download-Button auch ohne Bilder (fÃ¼r Beispiel-Daten)
    st.subheader("ğŸ’¾ Export-Optionen")
    st.info("Nach dem Hochladen und Verarbeiten von Bildern erscheint hier ein Download-Button fÃ¼r die JSON-Ergebnisse.")
    
    # Beispiel-Metadaten anzeigen
    with st.expander("â„¹ï¸ VerfÃ¼gbare Metadaten", expanded=False):
        st.markdown("""
        **Diese App kann folgende Metadaten extrahieren:**
        
        ğŸ“· **Kamera-Informationen:**
        - Hersteller und Modell
        - Objektiv
        - Software
        
        âš™ï¸ **Aufnahme-Einstellungen:**
        - Brennweite
        - Blende (f-number)
        - ISO-Wert
        - Belichtungszeit
        - WeiÃŸabgleich
        - Belichtungsmodus
        
        ğŸ•’ **Zeitstempel:**
        - Aufnahmedatum und -zeit
        - GPS-Zeitstempel
        
        ğŸ“ **Standort:**
        - GPS-Koordinaten
        - HÃ¶he Ã¼ber Meeresspiegel
        - VollstÃ¤ndige Adresse (mit Internetverbindung)
        
        ğŸ‘¥ **Gesichtsanalyse:**
        - Alter und Geschlecht
        - GesichtsqualitÃ¤t
        - Emotionen
        - Augen- und Mundstatus
        - Pose-SchÃ¤tzung
        """)
